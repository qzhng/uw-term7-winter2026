\documentclass[12pt]{article}
\usepackage[letterpaper, margin=1in]{geometry}

% PACKAGES
\usepackage{adjustbox}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{aliascnt}
\usepackage{bm}
\usepackage{braket}
\usepackage{empheq}
\usepackage{enumitem}
\usepackage{esint}
\usepackage{esvect}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{cleveref} % must be included after hyperref
\usepackage{siunitx}
\usepackage{tikz}
\usetikzlibrary{patterns, arrows.meta, calc, angles, quotes, decorations.pathreplacing, decorations.markings, positioning}
\usepackage[most]{tcolorbox}
\usepackage{pgfplots}
\usepgfplotslibrary{groupplots}
\pgfplotsset{compat=1.18}

% STATEMENT ENVIRONMENT
\newtheoremstyle{conditionalstyle}
  {3pt} % Space above
  {3pt} % Space below
  {\normalfont} % Body font - regular upright
  {} % Indent amount
  {\bfseries} % Theorem head font (only used when no optional argument)
  {.} % Punctuation after theorem head
  {.5em} % Space after theorem head
  {\thmnumber{\textbf{#1 #2}}\thmnote{\normalfont\textit{ (#3)}}} % Theorem head spec
\theoremstyle{conditionalstyle}
\newtheorem{definition}{Definition}[section]

% ALIAS FOR SHARED NUMBERING
\newaliascnt{axiom}{definition}
\newtheorem{axiom}[axiom]{Axiom}
\aliascntresetthe{axiom}

\newaliascnt{lemma}{definition}
\newtheorem{lemma}[lemma]{Lemma}
\aliascntresetthe{lemma}

\newaliascnt{theorem}{definition}
\newtheorem{theorem}[theorem]{Theorem}
\aliascntresetthe{theorem}

\newaliascnt{corollary}{definition}
\newtheorem{corollary}[corollary]{Corollary}
\aliascntresetthe{corollary}

\newaliascnt{note}{definition}
\newtheorem{note}[note]{Note}
\aliascntresetthe{note}

\newaliascnt{fact}{definition}
\newtheorem{fact}[fact]{Fact}
\aliascntresetthe{fact}

\newaliascnt{example}{definition}
\newtheorem{example}[example]{Example}
\aliascntresetthe{example}

% TCOLORBOX SETUP
\tcolorboxenvironment{definition}{
  breakable,
  enhanced,
  colback=teal!5!white,
  frame hidden,
  boxrule=0pt,
  arc=0pt, outer arc=0pt,
  left=5pt, % Padding so text doesn't touch the bar
  overlay={
    \draw[teal!75!black, line width=2pt] (frame.north west) -- (frame.south west);
  },
  before skip=10pt,
  after skip=10pt
}
\tcolorboxenvironment{axiom}{
  breakable, enhanced, colback=teal!5!white, frame hidden, boxrule=0pt,
  arc=0pt, outer arc=0pt, left=5pt,
  overlay={\draw[teal!75!black, line width=2pt] (frame.north west) -- (frame.south west);},
  before skip=10pt, after skip=10pt
}
\tcolorboxenvironment{theorem}{
  breakable, enhanced,
  colback=violet!5!white,
  frame hidden, boxrule=0pt,
  arc=0pt, outer arc=0pt,
  left=5pt,
  overlay={
    \draw[violet!75!black, line width=2pt] (frame.north west) -- (frame.south west);
  },
  before skip=10pt, after skip=10pt
}
\tcolorboxenvironment{lemma}{
  breakable, enhanced, colback=violet!5!white, frame hidden, boxrule=0pt,
  arc=0pt, outer arc=0pt, left=5pt,
  overlay={\draw[violet!75!black, line width=2pt] (frame.north west) -- (frame.south west);},
  before skip=10pt, after skip=10pt
}
\tcolorboxenvironment{corollary}{
  breakable, enhanced, colback=violet!5!white, frame hidden, boxrule=0pt,
  arc=0pt, outer arc=0pt, left=5pt,
  overlay={\draw[violet!75!black, line width=2pt] (frame.north west) -- (frame.south west);},
  before skip=10pt, after skip=10pt
}
\tcolorboxenvironment{fact}{
  breakable, enhanced, colback=violet!5!white, frame hidden, boxrule=0pt,
  arc=0pt, outer arc=0pt, left=5pt,
  overlay={\draw[violet!75!black, line width=2pt] (frame.north west) -- (frame.south west);},
  before skip=10pt, after skip=10pt
}
\tcolorboxenvironment{example}{
  breakable, enhanced,
  colback=gray!5!white,
  frame hidden, boxrule=0pt,
  arc=0pt, outer arc=0pt,
  left=5pt,
  overlay={
    \draw[gray!60!black, line width=2pt] (frame.north west) -- (frame.south west);
  },
  before skip=10pt, after skip=10pt
}
\tcolorboxenvironment{note}{
  breakable, enhanced,
  colback=orange!5!white,
  frame hidden, boxrule=0pt,
  arc=0pt, outer arc=0pt,
  left=5pt,
  overlay={
    \draw[orange!80!black, line width=2pt] (frame.north west) -- (frame.south west);
  },
  before skip=10pt, after skip=10pt
}
\newtcolorbox{important}[1][]{ % [1][] allows for an optional title override
  breakable,
  enhanced,
  colback=red!5!white,
  colframe=red!75!black,
  fonttitle=\bfseries,
  title={#1},
  before skip=10pt,
  after skip=10pt
}
\newtcolorbox{insight}[1][]{ % [1][] allows for an optional title override
  breakable,
  enhanced,
  colback=blue!5,
  colframe=blue!75,
  fonttitle=\bfseries,
  title={#1},
  before skip=10pt,
  after skip=10pt
}

% CLEVEREF ALIAS
\crefname{definition}{definition}{definitions}
\crefname{axiom}{axiom}{axioms}
\crefname{lemma}{lemma}{lemmas}
\crefname{theorem}{theorem}{theorems}
\crefname{corollary}{corollary}{corollaries}
\crefname{note}{note}{notes}
\crefname{fact}{fact}{facts}
\crefname{example}{example}{examples}

\crefalias{axiom}{axiom}
\crefalias{lemma}{lemma}
\crefalias{theorem}{theorem}
\crefalias{corollary}{corollary}
\crefalias{note}{note}
\crefalias{fact}{fact}
\crefalias{example}{example}

\Crefname{definition}{Definition}{Definitions}
\Crefname{axiom}{Axiom}{Axioms}
\Crefname{lemma}{Lemma}{Lemmas}
\Crefname{theorem}{Theorem}{Theorems}
\Crefname{corollary}{Corollary}{Corollaries}
\Crefname{note}{Note}{Notes}
\Crefname{fact}{Fact}{Facts}
\Crefname{example}{Example}{Examples}
\Crefname{equation}{Eq.}{Eqs.}

% BRACKETS TYPESET
\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}
\newcommand{\lb}{\left[}
\newcommand{\rb}{\right]}
\newcommand{\lc}{\left\{}
\newcommand{\rc}{\right\}}
\newcommand{\lv}{\lvert}
\newcommand{\rv}{\rvert}
\newcommand{\lV}{\lVert}
\newcommand{\rV}{\rVert}

% DELIMITER
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\inner}{\langle}{\rangle}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

% SET SPACE
\usepackage{setspace}
\onehalfspacing

% ---------- DOCUMENT ----------
\begin{document}

\pagenumbering{alph}
\begin{titlepage}
    \centering
    \vspace*{5cm} % Pushes the title down the page
    {\Large \textbf{AMATH 445}} \\[1em]
    {\Large \textbf{Scientific Machine Learning}} \\[1em]
    {\Large \textbf{Lecture Notes}} \\[1em]
    {\Large Winter 2026}
\end{titlepage}
\clearpage

\pagenumbering{roman}
\tableofcontents
\numberwithin{equation}{section}
\clearpage

\pagenumbering{arabic}

\section*{Lecture 1}
\addcontentsline{toc}{section}{Lecture 1}
\stepcounter{section}
\setcounter{section}{1}
\setcounter{equation}{0}

\subsection{What is Machine Learning}

\begin{definition}[Machine Learning]
  Machine learning is decision making based on data. The algorithm improves its performance on tasks based on experience (data).
\end{definition}

\begin{note}[Traditional Algorithm Workflow]
  \[
    \begin{rcases}
      \text{data/info} \\
      \text{recipe}
    \end{rcases} \rightarrow \text{traditional algorithm} \rightarrow \text{output}
  \]
\end{note}

\begin{note}[Machine Learning Workflow]
  \[
    \begin{rcases}
      \text{data/info} \\
      \text{output}
    \end{rcases} \rightarrow \text{ML} \rightarrow \text{recipe}
  \]
\end{note}

\begin{insight}[What, When and Why]
  \textbf{What} does ML do really well:
  \begin{itemize}[nosep, label=\tiny$\bullet$]
      \item Prediction: Forecasting unknown outcome (temperature).
      \item Representation: Finding structure in data (clustering).
      \item Decision making: Choosing the optimal action.
      \item Generation: ChatGPT, new data, image generation.
  \end{itemize}
  \textbf{When}
  \begin{itemize}[nosep, label=\tiny$\bullet$]
    \item Complex patterns: When relationships in data are too complex for traditional programming and we are not able to find the patterns ourselves.
    \item Large datasets: When there is an abundance of data that can be leveraged for learning.
  \end{itemize}

  \textbf{Why}
  \begin{itemize}[nosep, label=\tiny$\bullet$]
    \item It can generalize well to unseen data, making accurate predictions or decisions based on learned patterns (not memorize only).
  \end{itemize}
\end{insight}

\begin{important}[Important: ML and Data Quality]
  Machine Learning is only as good as the data it is trained on. Poor quality or biased data can lead to inaccurate or unfair outcomes. 
\end{important}

\subsection{Factors Driving Popularity of ML}

There are several factors driving the popularity of ML:
\begin{enumerate}[nosep, label=\arabic*.]
    \item Data availability.
    \item Parallel computing (GPU).
    \item Algorithm \& infrastructure maturity.
    \begin{itemize}[nosep, label=\tiny$\bullet$]
        \item CNN, RNN.
        \item Backward differentiation.
    \end{itemize}
\end{enumerate}

\subsection{Mechanics of ML}

The mechanics of ML is as follows
\begin{enumerate}[nosep]
\item Data representation $\rightarrow$ turn things into feature
    \item Pattern discovery $\rightarrow$ structure emerges from randomness
    \item Pick a model
    \item Training -- optimization
    \item Generalize $\rightarrow$ good model works on new data
    \item Evolution $\rightarrow$ test on new data (unseen)
\end{enumerate}

\subsection{Data}

\subsubsection{Data Splitting}

A common convention for data splitting is 70\% training, 15\% validation, and 15\% test.

\begin{important}[Important]
  TEST DATA is NEVER used in training!
\end{important}

\subsubsection{Label Availability}

This table shows the availability of features and outputs during the modeling phases versus real-world deployment.
\begin{center}
\begin{tabular}{l c c}
 & $F_1 \dots F_n$ & output \\
 \hline
 training & known & known \\
 validation & known & known \\
 test & known & known \\
 new & known & unknown
\end{tabular}
\end{center}

\subsection{Types of Error}

There are three types of error to track during model development:
\begin{itemize}[nosep, label=\tiny$\bullet$]
    \item Training error: error on the training set. It measures how well the model fits the training data.
    \item Validation error: error on the validation set. It guides hyperparameter tuning and model selection.
    \item Test error: error on the test set. It provides an unbiased estimate of model performance on unseen data.
\end{itemize}

\subsection{Training Dynamics}

As training progresses, training and validation errors evolve differently. The figure below illustrates the typical behavior.

\begin{center}
\begin{tikzpicture}[scale=1.2]
  % Axes
  \draw[->] (-0.2, 0) -- (8.8, 0) node[right] {Epochs};
  \draw[->] (0, -0.4) -- (0, 5.3) node[above left] {Error};

  % Training error (blue, monotonically decreasing)
  \draw[blue, thick, smooth, domain=0:8.3, samples=200]
    plot (\x, {4.2*exp(-0.55*\x) + 0.4});

  % Validation error (red, decreases then rises)
  \draw[red, thick, smooth, domain=0:8.3, samples=200]
    plot (\x, {3.5*exp(-0.65*\x) + 0.06*(\x)^2 + 0.55});

  % Early stop green dashed vertical line at sweet spot (x~3)
  \draw[green!60!black, dashed, thick] (3, 0) -- (3, 4.9);

  % Sweet Spot label
  \node[font=\small, above] at (3, 4.9) {Sweet Spot};

  % Early Stop label below x-axis
  \node[font=\small, green!60!black] at (3, -0.35) {Early Stop};

  % Underfitting annotation
  \node[font=\small] at (1.5, 4.2) {Underfitting};

  % Overfitting annotation
  \node[font=\small] at (6.3, 4.2) {Overfitting};

  % Legend
  \filldraw[blue] (5.4, 0.9) node[right, font=\small, blue] {Training Error};
  \filldraw[red]  (5.1, 2) node[right, font=\small, red]  {Validation Error};
\end{tikzpicture}
\end{center}

\begin{note}
Three key behaviors emerge from the training dynamics:
  \begin{itemize}[nosep, label=\tiny$\bullet$]
    \item \textbf{Underfitting}: Both training and validation errors are high. The model is too simple to capture the underlying patterns.
    \item \textbf{Overfitting}: Training error continues to decrease while validation error starts to increase. The model memorizes the training data but fails to generalize.
    \item \textbf{Early Stopping}: Stop training when the validation error begins to increase. This achieves the best generalization by stopping at the sweet spot.
  \end{itemize}
\end{note}

\subsubsection{Underfitting}

A model underfits when it is too simple (high bias) to capture the underlying structure of the data. Both training and validation errors remain high. Remedies include:
\begin{itemize}[nosep, label=\tiny$\bullet$]
    \item Increase model complexity.
    \item Add more or better features.
    \item Obtain more or better training data.
\end{itemize}

\subsubsection{Overfitting}

A model overfits when it learns the training data too closely — including its noise — and fails to generalize to new data. Training error is low but validation error is high. Remedies include:
\begin{itemize}[nosep, label=\tiny$\bullet$]
    \item Get more data.
    \item Regularization.
    \item Dropout.
    \item Batch normalization.
    \item Data augmentation.
    \item Early stopping.
\end{itemize}

\clearpage

\section*{Lecture 2}
\addcontentsline{toc}{section}{Lecture 2}
\stepcounter{section}
\setcounter{section}{2}
\setcounter{equation}{0}

\subsection{Categories of Machine Learning}

\subsubsection{Supervised Learning}

\begin{definition}[Supervised Learning]
  In supervised learning, the model is trained on a labeled dataset to learn a map
  \begin{equation}
    f : \mathbb{R}^d \to \mathcal{Y},
  \end{equation}
  where $\bm{X}_i \in \mathbb{R}^d$ is the feature vector of the $i$th data point (with $d$ features) and $y_i \in \mathcal{Y}$ is its label. The dataset consists of $N$ labeled pairs
  \begin{equation}
    \{(\bm{X}_i,\, y_i)\}_{i=1}^N,
  \end{equation}
  and the goal is to learn a mapping that generalizes to unseen data.
\end{definition}

Supervised learning is used for:
\begin{itemize}[nosep, label=\tiny$\bullet$]
    \item \textbf{Classification} (binary, multi-class or multi-label): Predict discrete class label.
    \item \textbf{Regression}: Predict continuous outputs (MSE, MAE are used commonly as loss function in regression).
\end{itemize}

\subsubsection{Unsupervised Learning}

\begin{definition}[Unsupervised Learning]
  The model is trained on unlabeled dataset:
  \begin{equation}
    \{\bm{X}_i\}^N_{i = 1}.
  \end{equation}
  The goal is to learn the underlying structure or distribution $p(\bm{X})$ of the data, without any label information.
\end{definition}

Common tasks include:
\begin{itemize}[nosep, label=\tiny$\bullet$]
  \item \textbf{Clustering:} partition the data into groups of similar points,
  \item \textbf{Dimensionality Reduction:} learn a mapping $f: \mathbb{R}^d \to \mathbb{R}^{d'}$ with $d' \ll d$ that preserves important structure,
  \item \textbf{Anomaly Detection:} identify points $\bm{X}_i$ where $p(\bm{X}_i) 
  \ll 1$.
\end{itemize}
\clearpage

\subsubsection{Reinforcement Learning}

The key elements of reinforcement learning are:
\begin{itemize}[nosep, label=\tiny$\bullet$]
  \item \textbf{Agent:} The learner or decision-maker that interacts with the 
  environment and learns to improve its behaviour over time.
  \item \textbf{Environment:} The external system the agent interacts with, which 
  responds to the agent's actions and produces new states and rewards.
  \item \textbf{State} $\bm{s}_t \in \mathcal{S}$\textbf{:} The current situation of 
  the agent in the environment, capturing all relevant information needed to make a 
  decision.
  \item \textbf{Action} $a_t \in \mathcal{A}$\textbf{:} The choices available to the 
  agent at each timestep, which can be discrete (e.g. legal moves in chess) or 
  continuous (e.g. joint angles of a robot arm).
  \item \textbf{Reward} $r_t$\textbf{:} A feedback signal received after each action, 
  indicating how successful that action was in progressing toward the goal.
  \item \textbf{Policy} $\pi : \mathcal{S} \to \mathcal{A}$\textbf{:} A strategy that 
  defines the agent's actions based on the current state, which the agent refines over 
  time to maximise cumulative reward.
\end{itemize}

\begin{definition}[Reinforcement Learning]
  In reinforcement learning, an agent learns a policy
  \begin{equation}
    \pi : \mathcal{S} \to \mathcal{A},
  \end{equation}
  where $\mathcal{S}$ is the state space and $\mathcal{A}$ is the action space. At 
  each timestep $t$, the agent: 
  \begin{enumerate}
    \item observes state $\bm{s}_t \in \mathcal{S}$,
    \item takes action $a_t \in \mathcal{A}$,
    \item receives reward $r_t$, generating a trajectory of tuples
    \begin{equation}
      \{(\bm{s}_t,\, a_t,\, r_t,\, \bm{s}_{t+1})\}_{t=0}^T.
    \end{equation}
  \end{enumerate}
  The goal is to learn a policy $\pi$ that maximizes the cumulative discounted reward
  \begin{equation}
    R = \sum_{t=0}^{T} \gamma^t r_t,
  \end{equation}
  where $\gamma \in [0,1]$ is a discount factor controlling the importance of future rewards.
\end{definition}

\begin{note}[State and Action Space]
  The state space $\mathcal{S}$ is the set of all possible situations the agent can 
  observe. For example, in a chess game $\mathcal{S}$ is the set of all possible board 
  configurations, while in a self-driving car $\mathcal{S}$ might encode the car's 
  position, speed, and surrounding obstacles.

  The action space $\mathcal{A}$ is the set of all possible actions the agent can take.
  The action space can be:
  \begin{itemize}[nosep, label=\tiny$\bullet$]
    \item \textbf{Discrete:} a finite set of distinct actions, 
    $\mathcal{A} = \{a_1, a_2, \ldots, a_K\}$. For example, in a chess game 
    $\mathcal{A}$ is the set of all legal moves.
    \item \textbf{Continuous:} an infinite range of actions, 
    $\mathcal{A} \subseteq \mathbb{R}^k$. For example, in a robot arm 
    $\mathcal{A}$ might be the set of all possible joint rotation angles.
  \end{itemize}
  Together, they define the full decision-making problem --- the policy 
  $\pi : \mathcal{S} \to \mathcal{A}$ maps each observed state to an action.
\end{note}


\subsubsection{Semi-supervised and Self-supervised Learning}

\begin{definition}[Semi-supervised Learning]
  In semi-supervised learning, the model is trained on a small set of labeled data
  \begin{equation}
    \{(\bm{X}_i,\, y_i)\}_{i=1}^{N_l},
  \end{equation}
  combined with a large set of unlabeled data
  \begin{equation}
    \{\bm{X}_j\}_{j=1}^{N_u}, \quad N_u \gg N_l.
  \end{equation}
  The model leverages the labeled data to learn initial patterns and then uses the unlabeled data to refine its understanding and improve generalization and performance.
\end{definition}

\begin{definition}[Self-supervised Learning]
  Self-supervised learning is a type of unsupervised learning in which the model generates its own pseudo-labels from the input data via a \emph{pretext task}. Given an unlabeled dataset
  \begin{equation}
    \{\bm{X}_i\}_{i=1}^N,
  \end{equation}
  the pretext task automatically constructs labeled training pairs $\{(\tilde{\bm{X}}_i,\, \hat{y}_i)\}_{i=1}^N$ by augmenting, masking, or transforming the original data, where $\hat{y}_i$ is the pseudo-label. The model then trains on these pairs in a supervised fashion, learning useful internal representations without requiring manual annotations.
\end{definition}

\begin{note}[Pretext Task]
  A pretext task is an artificial task designed to generate pseudo-labels from unlabeled data. The model does not ultimately care about solving the pretext task itself, the goal is to learn rich internal representations that can be transferred to downstream tasks. Common approaches include:
  \begin{itemize}[nosep, label=\tiny$\bullet$]
    \item \textbf{Masking:} Hide part of the input and train the model to predict the missing piece (e.g., masked language modeling in BERT).
    \item \textbf{Transformation prediction:} Apply a known transformation (e.g., rotation by $0^\circ, 90^\circ, 180^\circ, 270^\circ$) and train the model to predict which transformation was applied.
    \item \textbf{Next-token prediction:} Train the model to predict the next element in a sequence given all previous elements (e.g., GPT).
  \end{itemize}
\end{note}

\begin{note}[Self-supervised Learning vs.\ Unsupervised Learning]
  Both self-supervised and unsupervised learning operate on unlabeled data $\{\bm{X}_i\}_{i=1}^N$, but they differ in mechanism:
  \begin{itemize}[nosep, label=\tiny$\bullet$]
    \item \textbf{Unsupervised learning} discovers structure directly from the data distribution $p(\bm{X})$ without any labels (e.g., clustering, dimensionality reduction).
    \item \textbf{Self-supervised learning} manufactures pseudo-labels via a pretext task and then trains in a supervised fashion on the resulting (input, pseudo-label) pairs.
  \end{itemize}
  In other words, unsupervised learning finds patterns without labels, while self-supervised learning creates its own labels and uses the supervised learning framework to learn representations.
\end{note}

\begin{note}[Integrating Mechanistic Models with ML]
  Integrating Mechanistic Models with Machine Learning: Using physical laws or domain knowledge to inform and constrain machine learning models, enhancing their interpretability and reliability.
\end{note}

\subsection{Supervised Learning and Classification}

\subsubsection{The Problem Statement of Classification}

The input (feature vector) is $\bm{X} \in \mathbb{R}^d$, the output (class label) is a discrete variable $y \in \{1, 2, 3, \dots, K\}$. Given a new $\bm{X}_\text{new}$, the model should predict which class it belongs to, $y_\text{new}$.

\subsubsection{A Probabilistic View of Classification}

\begin{definition}[Posterior Probability]
\label{Posterior Probability}
    The posterior probability is defined as
    \begin{equation}
      P(y = k \mid \bm{X}),
    \end{equation}
    where $y = k$ is the hypothesis and $\bm{X}$ is the data.
\end{definition}

\begin{insight}[Posterior Probability vs. Conditional Probability]
  The distinction between conditional and posterior probability is not mathematical, it is conceptual. Posterior emphasizes learning from data, while conditional is a neutral probabilistic relationship.
\end{insight}

We want to minimize the probability of misclassifications.

\begin{theorem}[Bayes Decision Rule]
\label{Bayes Decision Rule}
    The Bayes classifier assigns an input $\bm{X}$ to the class with largest posterior probability
    \begin{equation}
      \hat{y}(\bm{X}) = \operatorname*{arg\,max}_{k \in \{ 1, 2, \dots , K\}} P(y = k \mid \bm{X}).
    \end{equation}
\end{theorem}

\begin{theorem}[Bayes Theorem]
\label{bayes theorem}
    The Bayes theorem is
    \begin{equation}
      P(y = k \mid \bm{X}) = \frac{P(\bm{X} \mid y = k)\, P(y = k)}{P(\bm{X})}.
    \end{equation}
    It flip which variable is being conditioned on.
\end{theorem}

\begin{definition}
  The conditional probabilit is given by
  \begin{equation}
    P(A \mid B) = \frac{P(A \cap B)}{P(B)}.
  \end{equation}
\end{definition}

\begin{note}[Posterior, Likelihood, and Conditional Probability]
  All three are conditional probabilities $P(A \mid B)$, but differ in context:
  \begin{itemize}[nosep, label=\tiny$\bullet$]
    \item \textbf{Conditional probability:} The general mathematical concept $P(A \mid B)$, with no particular interpretation attached.
    \item \textbf{Posterior probability} $P(y = k \mid \bm{X})$\textbf{:} A conditional probability where we condition on \emph{data} to learn about a \emph{hypothesis}. ``Posterior'' means \emph{after} --- it is our belief about the class \emph{after} observing the evidence.
    \item \textbf{Likelihood} $P(\bm{X} \mid y = k)$\textbf{:} A conditional probability where we condition on a \emph{hypothesis} to evaluate how well it explains the \emph{data}.
    \item \textbf{Prior probability} $P(y = k)$\textbf{:} Our belief about the class \emph{before} observing any data.
  \end{itemize}
  The distinction is conceptual, not mathematical. Bayes Theorem connects them:
  \[
    \underbrace{P(y = k \mid \bm{X})}_{\text{posterior}} = \frac{\overbrace{P(\bm{X} \mid y = k)}^{\text{likelihood}} \cdot \overbrace{P(y = k)}^{\text{prior}}}{\underbrace{P(\bm{X})}_{\text{evidence}}}.
  \]
\end{note}

Using \cref{Bayes Decision Rule} and \cref{bayes theorem}, since the denominator is $k$ independent, the Bayes decision rule is implied to be
\begin{equation}
\label{bayes classifier with bayes theorem}
\hat{y}(\bm{X}) = \operatorname*{arg\,max}_{k \in \{ 1, 2, \dots , K\}} P(\bm{X} \mid y = k)\, P(y = k),
\end{equation}
where $P(\bm{X} \mid y = k)$ is the class-conditional likelihood and $P(y = k)$ is the prior probability of class $k$.

\subsubsection{Linear Discriminant Analysis (LDA)}

LDA is based on the following assumptions:
\begin{itemize}[nosep, label=\tiny$\bullet$]
    \item $P(\bm{X} \mid y = k) = \mathcal{N}(\bm{X} \mid \bm{\mu}_k, \Sigma)$, a multivariate Gaussian.
    \item All classes share the same covariance matrix $\Sigma_k = \Sigma$.
    \item $\Pi_k = P(y = k)$, the prior probability.
    \item $\bm{\mu}_k \in \mathbb{R}^d$ is the mean vector of class $k$.
\end{itemize}

Therefore
\begin{equation}
    P(\bm{X} \mid y = k)\, P(y = k) = P(\bm{X} \mid y = k)\,\Pi_k,
\end{equation}
so that \cref{bayes classifier with bayes theorem} is
\begin{equation}
\label{bayes classifier with LDA}
    \hat{y}(\bm{X}) = \operatorname*{arg\,max}_{k \in \{ 1, 2, \dots , K\}} P(\bm{X} \mid y = k)\,\Pi_k.
\end{equation}
Taking the log of \cref{bayes classifier with LDA}
\begin{equation}
    \hat{y}(\bm{X}) = \operatorname*{arg\,max}_{k \in \{ 1, 2, \dots , K\}} \lp \log P(\bm{X} \mid y = k) + \log \Pi_k \rp.
\end{equation}
Given that
\[
P(\bm{X} \mid y=k) = \frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} \exp\!\left( -\frac{1}{2} (\bm{X} - \bm{\mu}_k)^\top \Sigma^{-1} (\bm{X} - \bm{\mu}_k) \right).
\]
Calculate $\log P(\bm{X} \mid y = k)$ to get
\begin{equation}
\label{log of posterior of X y=k}
    \log P(\bm{X} \mid y = k)
    =
    -\frac{1}{2} \bm{X}^{\top}\Sigma^{-1} \bm{X}
    + \bm{X}^{\top}\Sigma^{-1}\bm{\mu}_k
    - \frac{1}{2}\bm{\mu}_k^{\top}\Sigma^{-1}\bm{\mu}_k
    - \frac{1}{2}\log|\Sigma|
    -\frac{d}{2}\log(2\pi).
\end{equation}
Substitute into $\hat{y}(\bm{X})$ and drop terms that are independent of $k$ to obtain the LDA
\[
\hat{y}(\bm{X})
=
\arg\max_k \, \delta_k(\bm{X})
\]
where
\[
\delta_k(\bm{X})
=
\bm{X}^{\top}\Sigma^{-1}\bm{\mu}_k
- \frac{1}{2}\bm{\mu}_k^{\top}\Sigma^{-1}\bm{\mu}_k
+ \log \Pi_k.
\]

\begin{definition}[LDA]
\label{lda}
    The LDA is defined as
    \[
    \hat{y}(\bm{X}) = \operatorname*{arg\,max}_{k \in \{ 1, 2, \dots , K\}} \delta_k(\bm{X}),
    \]
    where $\delta_k$ is defined as
    \[
    \delta_k(\bm{X}) = \bm{X}^\top \Sigma^{-1} \bm{\mu}_k - \frac{1}{2}\bm{\mu}_k^\top \Sigma^{-1} \bm{\mu}_k + \log \Pi_k.
    \]
\end{definition}

Suppose we have a case where $K = 2$, i.e., two classes
\[
\delta_1(\bm{X}) - \delta_2(\bm{X}) \ge 0.
\]
Substitute from
\[
\delta_k(\bm{X}) \;\Longrightarrow\; \bm{W}^\top \bm{X} + b \ge 0
\]
Then
\[
\bm{W} = \Sigma^{-1}(\bm{\mu}_1 - \bm{\mu}_2),
\quad
b = -\frac{1}{2}
\left(
\bm{\mu}_1^\top \Sigma^{-1} \bm{\mu}_1
-
\bm{\mu}_2^\top \Sigma^{-1} \bm{\mu}_2
\right)
+ \log\frac{\Pi_1}{\Pi_2}
\]

\subsubsection{Logistic Regression}

Logistic Regression is a supervised algorithm for classification. We approximate the posterior probability $P(y = k \mid \bm{X})$.

The problem we solve is a binary classification. We have features $\bm{X} \in \mathbb{R}^d$ and $y \in \{0, 1\}$ as outputs. The goal is to model the probability that an observation belongs to class $1$, given its features, i.e., we need
\[
P(y = 1 \mid \bm{X}).
\]

Recall in linear regression, we try to fit a line to data or hyperplane to higher dimension data with
\[
\bm{\beta}^\top \bm{X}, \quad \beta_0+\beta_1 x_1 + \dots + \beta_{d-1} x_d, \quad \bm{\beta} \in \mathbb{R}^d
\]
for
\[
\bm{X} =
\begin{bmatrix}
    1 \\
    x_1 \\
    \vdots \\
    x_d
\end{bmatrix}.
\]
To obtain results as probability, we need to transform with a sigmoid function
\[
\bm{\beta}^\top \bm{X} \rightarrow \sigma\lp \bm{\beta}^\top \bm{X} \rp = \frac{1}{1 + e^{-\bm{\beta}^\top \bm{X}}}.
\]

\begin{center}
    \begin{tikzpicture}[scale=1.1]
      % Axes
      \draw[->] (-4.2,0) -- (4.5,0) node[below right] {$z$};
      \draw[->] (0,-0.2) -- (0,3.2) node[above left] {$\sigma(z)$};
    
      % Sigmoid curve: y = 3/(1+e^{-z}) so it asymptotes to 3 (nice for drawing)
      \draw[thick, smooth, domain=-4:4, samples=200]
        plot (\x,{3/(1+exp(-\x))});
    
      % Dashed asymptote at 1 (in the board it's at the top; here we draw it at y=3 then label "1")
      \draw[dashed] (-4.2,3) -- (4.2,3);
      \node[left] at (0,3) {$1$};
    
      % Mark sigma(0)=1/2
      \fill (0,1.5) circle (1.2pt);
      \node[left] at (0,1.8) {$\tfrac{1}{2}$};
    
      % Origin label
      \node[below left] at (0,0) {$0$};
    \end{tikzpicture}
\end{center}

The new probability after the sigmoid transformation is
\[
\begin{cases}
    P(y = 1 \mid \bm{X}) = \dfrac{1}{1 + e^{-\bm{\beta}^\top \bm{X}}} \\[6pt]
    P(y = 0 \mid \bm{X}) = 1 - P(y = 1 \mid \bm{X}).
\end{cases}
\]
Likelihood function:
Given the dataset $\{(\bm{X}_i, y_i)\}_{i=1}^N$ and $y \in \{0,1\}$, the likelihood function $L(\bm{\beta})$ is the product of the probabilities of observing each data point:
\[
P(y_i \mid \bm{X}_i, \bm{\beta})
=
\bigl(P(y = 1 \mid \bm{X}_i)\bigr)^{y_i}
\bigl(P(y = 0 \mid \bm{X}_i)\bigr)^{1 - y_i}
\]
\[
L(\bm{\beta})
=
\prod_{i=1}^N P(y_i \mid \bm{X}_i, \bm{\beta})
=
\prod_{i=1}^N
\bigl(\sigma(\bm{\beta}^\top \bm{X}_i)\bigr)^{y_i}
\bigl(1 - \sigma(\bm{\beta}^\top \bm{X}_i)\bigr)^{1 - y_i}
\]
We work with the log-likelihood function
\[
\hat{L}(\bm{\beta}) = \sum_{i = 1}^N \lb y_i \log \sigma(\bm{\beta}^\top \bm{X}_i) + (1 - y_i)\log \lp 1 - \sigma(\bm{\beta}^\top \bm{X}_i) \rp \rb,
\]
where $\sigma$ is the sigmoid function.

It is difficult to find a closed-form solution like the LDA, so gradient-descent etc. will be used.
\clearpage

\section*{Lecture 3}
\addcontentsline{toc}{section}{Lecture 3}
\stepcounter{section}
\setcounter{section}{3}
\setcounter{equation}{0}

\subsection{Review And Summary of Lecture 2}

\subsubsection{Bayes Decision Rule}

Let \(y \in \{0,1\}\) denote the class label and \(\bm X\) the feature vector. The Bayes classifier is given by
\[
\hat{y}(\bm X)
= \arg\max_{k} \, \mathbb{P}(y = k \mid \bm X),
\]
or using Bayes' theorem, this can be written as
\[
\hat{y}(\bm X)
= \arg\max_{k} \, \mathbb{P}(\bm X \mid y = k)\,{\mathbb{P}(y = k)},
\]
where \(\mathbb{P}(\bm X \mid y = k)\) is the class-conditional likelihood and \(\mathbb{P}(y = k) = \Pi_k\) is the prior probability of class \(k\).

\subsubsection{Linear Discriminant Analysis (LDA)}

In Linear Discriminant Analysis (LDA), it is assumed that
\[
\mathbb{P}(\bm X \mid y = k)
\]
is Gaussian for each class \(k\), with all classes sharing a common covariance matrix.

\subsubsection{Logistic Regression}

The logistic regression
\[
\mathbb P(y = 1 \mid X) = \sigma(\bm \beta^T \bm X) = \frac{1}{1 + e^{-\bm \beta^T \bm X}}
\]
directly models the posterior probability and is typically used for binary classification problems with
\[
y \in \{0,1\},
\]
and labeled data
\[
\lc (\bm X_i, y_i) \rc_{i=1}^n.
\]

\subsubsection{Log-Likelihood and Loss Function}

The likelihood formulation is
\[
L(\bm \beta)
= \prod_{i=1}^{n}
\bigl(\sigma(\bm \beta^T \bm X_i)\bigr)^{y_i}
\bigl(1 - \sigma(\bm \beta^T \bm X_i)\bigr)^{1 - y_i}.
\]
We want to choose \(\bm \beta\) that maximizes the log-likelihood:
\[
\log L(\bm \beta)
= \sum_{i=1}^{n}
\Bigl(
y_i \log \sigma(\bm \beta^T \bm X_i)
+ (1 - y_i)\log\bigl(1 - \sigma(\bm \beta^T \bm X_i)\bigr)
\Bigr).
\]
Equivalently, we minimize the negative log-likelihood, which defines the
binary cross-entropy loss
\[
J(\bm \beta) = - \log L(\bm \beta).
\]
The binary cross-entropy loss $J(\beta)$ has no closed-form solution. We therefore turn to iterative optimization methods. For this course, we use gradient descent. 

\subsection{Gradient Descent}

\subsubsection{General Update Rule}

Suppose
\[
\min_{\beta \in \mathbb R^d} f(\bm \beta) \quad f: \mathbb R^d \to \mathbb R,
\]
where \(f\) is differentiable. We need to find \(\bm \beta^*\) such that
\[
\bm \nabla f(\bm \beta^*) = 0.
\]
The directional derivative is minimized when
\[
\hat{\bm u} = -\frac{\bm \nabla f(\bm \beta^*)}{\norm {\bm \nabla f (\bm \beta^*)}}.
\]
The basic idea is to use gradient descent to minimize the loss function
\[
f \left(\bm \beta^{(k)} + \Delta \bm \beta\right)
\simeq
f \left(\bm \beta^{(k)}\right)
+ \underbrace{\nabla f \left(\bm \beta^{(k)}\right)^T \Delta \bm \beta}_{< 0},
\]
where
\[
\Delta \bm \beta = -\eta \, \nabla f \left(\bm \beta^{(k)}\right).
\]
The parameter update rule is
\begin{equation}
\label{update rule}
\bm \beta^{(k+1)} = \bm \beta^{(k)} - {\eta} \, \underbrace{\bm \nabla f \left(\bm \beta^{(k)}\right)}_{\equiv \bm \nabla J(\bm \beta)}.
\end{equation}
where \(\eta\) is the learning rate.

\subsubsection{Gradient Descent for Logistic Regression}

Applying \cref{update rule} to logistic regression requires computing the gradient of $J(\bm \beta)$ with respect to $\bm \beta$. We differentiate the binary cross-entropy loss term by term. We need
\[
\bm \nabla_{\bm \beta} J(\bm \beta)
= - \sum_{i=1}^{n}
\nabla_{\bm \beta}
\Bigl[
y_i \log \sigma(\bm \beta^{T} \bm X_i)
+ (1 - y_i)\log \bigl(1 - \sigma(\bm \beta^{T} \bm X_i)\bigr)
\Bigr].
\]
Let
\[z_i = \bm \beta^T \bm X_i,
\quad
\frac{d}{dz}\log \sigma(z) = 1 - \sigma(z),
\quad
\frac{d}{dz}\log \bigl(1 - \sigma(z)\bigr) = -\sigma(z),
\quad
\bm \nabla_{\bm \beta} z_i = \bm X_i,
\]
then,
\begin{equation*}
    \left.
    \begin{array}{r}
        \nabla_{\bm \beta} \bigl[y_i \log \sigma(z_i)\bigr] = y_i \bigl(1 - \sigma(z_i)\bigr) \bm X_i \\[4pt]
        \nabla_{\bm \beta} \bigl[(1 - y_i)\log \bigl(1 - \sigma(z_i)\bigr)\bigr] = - (1 - y_i)\sigma(z_i) \bm X_i       
    \end{array}
    \right\}
    \;\Longrightarrow \; \bm \nabla_{\bm \beta} J(\bm \beta) = \sum_{i=1}^{n} \bigl(\sigma(z_i) - y_i\bigr) \bm X_i.
\end{equation*}
Note that the update rule is now
\[
\bm \beta^{(k+1)} = \bm \beta^{(k)} - \eta \, \bm \nabla_{\bm \beta} J(\bm \beta^{(k)}).
\]

\subsection{Example: Ising Model}

To illustrate classification in a physics context, consider the Ising model phase transition. Given a spin configuration, can we classify whether the system is above or below the critical temperature?

The Hamiltonian of the system is
\[
H = -J \sum_{\langle i j\rangle} \sigma_i \sigma_j ,
\]
where \(\langle ij\rangle\) denotes nearest-neighbor pairs (2D lattice) and \(\sigma_i \in \{-1,+1\}\). For \(J=1\) in the 2D square-lattice Ising model: \(T_c \approx 2.269\).
\begin{center}
\begin{tikzpicture}[scale=0.9, every node/.style={font=\small}]
  % Parameters
  \def\dx{0.8}      % spacing
  \def\sep{5.0}     % separation between panels
  \def\L{0.6}       % arrow length
  \def\anchor{0.5}  
  \def\xoff{0.4}    
  \def\yoff{0.8}    
  
  % Titles / labels
  \node at (1.2,3.8) {$T<T_c$};
  \node at (1.2,-0.6) {Order};
  \node at (\sep+1.2,3.8) {2D lattice $T>T_c$};
  \node at (\sep+1.2,-0.6) {Disorder};
  \node at (\sep/2+1.2,3.8) {$T_c$};
  % Separator (phase boundary)
  \draw (\sep/2+1.2,-0.1) -- (\sep/2+1.2,3.3);
  
  % --- LEFT panel: ordered (all down) ---
  \begin{scope}[shift={(\xoff,\yoff)}]
    \foreach \i in {0,...,2}{
      \foreach \j in {0,...,2}{
        \draw[->, line width=0.5pt, line cap=round]
          (\i*\dx,\j*\dx+\anchor*\L) -- ++(0,-\L);
      }
    }
  \end{scope}
  
  % --- RIGHT panel: disordered (mixed) ---
  \begin{scope}[shift={(\sep+\xoff,\yoff)}]
    % Up arrows
    \foreach \i/\j in {0/0, 2/0, 1/1, 0/2, 1/2}{
      \draw[->, line width=0.5pt, line cap=round]
        (\i*\dx,\j*\dx-\anchor*\L) -- ++(0,\L);
    }
    % Down arrows
    \foreach \i/\j in {1/0, 0/1, 2/1, 2/2}{
      \draw[->, line width=0.5pt, line cap=round]
        (\i*\dx,\j*\dx+\anchor*\L) -- ++(0,-\L);
    }
  \end{scope}
\end{tikzpicture}
\end{center}

In this setting, the feature vector $\bm X$ could consist of statistics computed from the spin configuration, such as the magnetization, energy, or spatial correlation functions. The label $y \in \{0,1\}$ indicates whether the configuration was sampled from the ordered phase ($T < T_c$) or the disordered phase ($T > T_c$). Logistic regression can then be trained on labeled configurations to predict the phase from the features.

\subsection{Example: Prediction of Immunotherapy Response}

Consider the problem of predicting patient response to immunotherapy with pre-treatment clinical data. This example is motivated by recent work on LORIS Immunotherapy, reported in
Nature Cancer (2024).

\emph{Question}: Can we predict, prior to treatment, the probability that a patient will respond to immunotherapy using measured clinical variables?

We are given labeled clinical data consisting of six measured features per patient. Let
\[
\bm X \in \mathbb{R}^6
\]
denote the feature vector for a single patient, and let
\[
\{(\bm X_i, y_i)\}_{i=1}^n
\]
be the full dataset, where \(n\) is the number of data points and
\[
y_i \in \{0,1\}
\]
indicates whether patient \(i\) responds to immunotherapy. The goal is to model the probability of response given the clinical features.

We use a logistic regression model to estimate the probability of response:
\[
\mathbb{P}(y = 1 \mid \bm X)
= \sigma \left(\bm \beta^{T} \bm X\right)
= \sigma \left(
\beta_0 + \sum_{j=1}^{6} \beta_j X_j
\right),
\]
where \(\beta_0\) is the bias (intercept) term. The quantity
\[
\mathbb{P}(y = 1 \mid \bm X)
\]
represents the predicted probability that a patient responds to immunotherapy given their pre-treatment clinical variables.

\subsubsection{Log-Odds of Response Approach}

An equivalent and often more interpretable form of logistic regression is obtained by considering the \emph{odds} and \emph{log-odds} of response. The odds of response given the clinical features \(\bm X\) are defined as
\[
\frac{\mathbb{P}(y = 1 \mid \bm X)}{\mathbb{P}(y = 0 \mid \bm X)}
= \frac{\mathbb{P}(y = 1 \mid \bm X)}{1 - \mathbb{P}(y = 1 \mid \bm X)}.
\]
Using the logistic regression model
\[
\mathbb{P}(y = 1 \mid \bm X)
= \sigma \left(\beta_0 + \sum_{j=1}^{6} \beta_j X_j \right),
\]
we obtain the log-odds:
\begin{align}
\log \frac{\mathbb{P}(y = 1 \mid \bm X)}{\mathbb{P}(y = 0 \mid \bm X)}
&= \log \frac{\mathbb{P}(y = 1 \mid \bm X)}{1 - \mathbb{P}(y = 1 \mid \bm X)} \notag \\
&= \beta_0 + \sum_{j=1}^{6} \beta_j X_j.
\end{align}

\clearpage

\section*{Lecture 4}
\addcontentsline{toc}{section}{Lecture 4}
\stepcounter{section}
\setcounter{section}{4}
\setcounter{equation}{0}

\subsection{Support Vector Machines}

SVM is a form of supervised learning, we have labeled data
\[
\lc (\bm X_i, y_i) \rc^n_{i=1}.
\]

(FILL IN)

We have a classifier
\[
f(x) = \omega^T \bm X + b
\]
that outputs
\[
\operatorname{sign}f(\bm X).
\]
We have a decision boundary
\[
\omega^T \bm X + b = 0.
\]
Note that
\begin{itemize}[nosep, label=\tiny$\bullet$]
    \item \(\bm \omega\) is the normal vector to this decision boundary.
    \item 
\end{itemize}

\end{document}

% ---------- EXTRA COMMANDS ----------
% LIST
[nosep, leftmargin=*]
[nosep, label=\tiny$\bullet$]

% ENUMERATE LABEL TO ABC
[label(breaking lable in cref)=(\alph*)]

% INSERT MEDIA
\includegraphics[width=\linewidth]{}

% MINI PAGE 
\begin{minipage}[t]{\linewidth}
    \begin{center}
    \adjustbox{valign=t}{
    \includegraphics[width=0.5\linewidth]{q6b.jpeg}
    }
    \end{center}
\end{minipage}
