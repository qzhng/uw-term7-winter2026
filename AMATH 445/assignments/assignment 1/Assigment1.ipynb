{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14c5b872",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883f95f9",
   "metadata": {},
   "source": [
    "#### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea703202",
   "metadata": {},
   "source": [
    "##### (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf724bb",
   "metadata": {},
   "source": [
    "Precision measures the exactness of the model. It is the ratio of correctly predicted positive observations to the total predicted positives: $$\\text{Precision} = \\frac{TP}{TP + FP}.$$ In the context of binary classification, precision is how reliable the model is when it predicts the positive class.\n",
    "\n",
    "Recall measures the completeness of the model. It is the ratio of correctly predicted positive observations to all actual positives: $$\\text{Recall} = \\frac{TP}{TP + FN}.$$ Recall is the model's ability to find all relevant cases within the dataset. A high recall indicates that the model has a low False Negative (FN) rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef184999",
   "metadata": {},
   "source": [
    "##### (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dabd6d",
   "metadata": {},
   "source": [
    "Accuracy can be misleading because it only measures the overall fraction of correct predictions without accounting for the distribution of classes or the specific types of errors made. Precision and recall are essential complementary metrics because they reveal the specific trade-offs of the model. For example, if 98% of a dataset belongs to the negative class, a model that simply predicts \"negative\" for every single instance will achieve 98% accuracy despite failing to identify any positive cases (0% recall)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9c02f0",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0177159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def encode_binary_columns(df):\n",
    "    df_encoded = df.copy()\n",
    "\n",
    "    obj_cols = df_encoded.select_dtypes(include=['object', 'category', 'string']).columns\n",
    "    \n",
    "    for col in obj_cols:\n",
    "        unique_vals = df_encoded[col].dropna().unique()\n",
    "        if len(unique_vals) <= 2:\n",
    "            unique_vals = sorted(unique_vals)\n",
    "            mapping = {val: i for i, val in enumerate(unique_vals)}\n",
    "            df_encoded[col] = df_encoded[col].map(mapping)\n",
    "    return df_encoded\n",
    "\n",
    "df = pd.read_csv('Q1_bioprinting_data.csv')\n",
    "\n",
    "# I followed the threshold mentioned in the paper to determine viability.\n",
    "VIABILITY_THRESHOLD = 80 \n",
    "y = (df['Viability_at_time_of_observation_(%)'] >= VIABILITY_THRESHOLD).astype(int)\n",
    "\n",
    "cols_to_drop = [\n",
    "    'Viability_at_time_of_observation_(%)', \n",
    "    'Reference', \n",
    "    'DOI', \n",
    "    'Acceptable_Viability_(Yes/No)',\n",
    "    'Acceptable_Pressure_(Yes/No)'\n",
    "]\n",
    "df = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "df['Syringe_Temperature_(째C)'] = df['Syringe_Temperature_(째C)'].fillna(22)\n",
    "df['Substrate_Temperature_(째C)'] = df['Substrate_Temperature_(째C)'].fillna(22)\n",
    "\n",
    "cols_only_null_zero = [\n",
    "    col for col in df.columns \n",
    "    if ((df[col].isna()) | (df[col] == 0)).all()\n",
    "]\n",
    "df = df.drop(columns=cols_only_null_zero)\n",
    "\n",
    "df = df.loc[:, df.isna().mean() <= 0.5]\n",
    "\n",
    "df = encode_binary_columns(df)\n",
    "\n",
    "df_numeric = df.select_dtypes(include=['number'])\n",
    "imputer = KNNImputer(n_neighbors=30)\n",
    "df_imputed_data = imputer.fit_transform(df_numeric)\n",
    "df_imputed = pd.DataFrame(df_imputed_data, columns=df_numeric.columns)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled_array = scaler.fit_transform(df_imputed)\n",
    "X = pd.DataFrame(df_scaled_array, columns=df_numeric.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbd5382",
   "metadata": {},
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2bcb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Accuracy: 0.7339\n",
      "Precision: 0.7792\n",
      "Recall: 0.7895\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=67)\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state=67)\n",
    "\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_dt = dt_clf.predict(X_test)\n",
    "\n",
    "dt_acc = accuracy_score(y_test, y_pred_dt)\n",
    "dt_prec = precision_score(y_test, y_pred_dt)\n",
    "dt_rec = recall_score(y_test, y_pred_dt)\n",
    "\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Accuracy: {dt_acc:.4f}\")\n",
    "print(f\"Precision: {dt_prec:.4f}\")\n",
    "print(f\"Recall: {dt_rec:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a66d99d",
   "metadata": {},
   "source": [
    "Accuracy (0.7339): the model correctly classifies approximately 73.4% of the bio-printing outcomes. This indicates a moderate ability to distinguish between acceptable and unacceptable viability based on the parameters.\n",
    "\n",
    "Precision (0.7792): the model is fairly reliable when it predicts a positive outcome. This means that when the DT claims a bio-ink is \"Acceptable,\" it is correct the majority of the time, keeping False Positives relatively low.\n",
    "\n",
    "Recall (0.7895): the model achieves a recall of 79.0%, meaning it successfully identifies most of the actually viable bio-inks, though it still misses about 21% of them (False Negatives)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df4b6b6",
   "metadata": {},
   "source": [
    "#### (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce63e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "Accuracy: 0.7258\n",
      "Precision: 0.7625\n",
      "Recall: 0.8026\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf = SVC(random_state=67)\n",
    "\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_svm = svm_clf.predict(X_test)\n",
    "\n",
    "svm_acc = accuracy_score(y_test, y_pred_svm)\n",
    "svm_prec = precision_score(y_test, y_pred_svm)\n",
    "svm_rec = recall_score(y_test, y_pred_svm)\n",
    "\n",
    "print(\"SVM\")\n",
    "print(f\"Accuracy: {svm_acc:.4f}\")\n",
    "print(f\"Precision: {svm_prec:.4f}\")\n",
    "print(f\"Recall: {svm_rec:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1008c1",
   "metadata": {},
   "source": [
    "Accuracy (0.7258): the model correctly classifies approximately 72.6% of the bio-printing outcomes. This indicates a moderate ability to distinguish between acceptable and unacceptable viability based on the parameters.\n",
    "\n",
    "Precision (0.7625): the model is fairly reliable when it predicts a positive outcome. This means that when the SVM claims a bio-ink is \"Acceptable,\" it is correct the majority of the time, keeping False Positives relatively low.\n",
    "\n",
    "Recall (0.8026): the model achieves a recall of 80.0%, meaning it successfully identifies most of the actually viable bio-inks, though it still misses about 20% of them (False Negatives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "249df87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Comparison\n",
      "   Metric     DT    SVM\n",
      " Accuracy 0.7339 0.7258\n",
      "Precision 0.7792 0.7625\n",
      "   Recall 0.7895 0.8026\n"
     ]
    }
   ],
   "source": [
    "# Model Comparison\n",
    "print(\"Model Comparison\")\n",
    "models = [\"DT\", \"SVM\"]\n",
    "accs = [dt_acc, svm_acc]\n",
    "precs = [dt_prec, svm_prec]\n",
    "recs = [dt_rec, svm_rec]\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\"],\n",
    "    \"DT\": [dt_acc, dt_prec, dt_rec],\n",
    "    \"SVM\": [svm_acc, svm_prec, svm_rec]\n",
    "})\n",
    "print(comparison_df.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8312b8c7",
   "metadata": {},
   "source": [
    "The decision tree is slightly better at being precise: minimizing failed experiments predicted as successes, resulting in a marginally higher overall accuracy. The SVM has better recall, making it the better choice if the primary goal is to find as many viable bio-ink candidates as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edefee3f",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6ee4d0",
   "metadata": {},
   "source": [
    "#### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed3c990",
   "metadata": {},
   "source": [
    "Unregularized Logistic Regression fails with linearly separable data because it tries to achieve perfect classification with 100% confidence ($P=1$). Since the sigmoid function $\\sigma(z)$ only reaches exactly 1 when the input $z$ is infinity, the model keeps increasing the magnitude of the weights $\\boldsymbol{\\beta}$ forever to push the score higher. Without a regularization penalty to stop this growth, the weights explode towards infinity, and the algorithm never actually converges to a final solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4dc061",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c764ff",
   "metadata": {},
   "source": [
    "The log-loss (term 1) decreases as weights grow, the penalty (term 2) grows quadratically. Because $\\lambda > 0$, the penalty eventually outgrows the decay of the log-loss. This forces the total loss $J(\\boldsymbol{\\beta})$ to approach $+\\infty$ as the weights grow in any direction. Since $J$ is continuous and a continuous function that goes to infinity in all directions must have a minimum, this guarantees the existence of a global minimum at a finite value of $\\boldsymbol{\\beta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b5a69e",
   "metadata": {},
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e4b514",
   "metadata": {},
   "source": [
    "A larger $\\lambda$ forces the model to be simpler (smaller weights). The bias increases which may prevent the model from capturing complex underlying patterns in the training data and cause underfitting. The variance decreases, the model becomes less sensitive to noise in the training set, leading to more stable predictions on new data and reduces overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58544e02",
   "metadata": {},
   "source": [
    "#### (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f05db9",
   "metadata": {},
   "source": [
    "Lowering threshold from 0.5 to 0.3 will cause:\n",
    "\n",
    "FNR to decrease: The model correctly identifies more true positives, so fewer positive cases are missed (higher recall). FPR to increase: The model is more likely to incorrectly label negative instances as positive.\n",
    "\n",
    "This is beneficial when False Negatives are costly or when the goal is to identify as many positive instances as possible (high recall), even at the cost of more false alarms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8953e17f",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76803445",
   "metadata": {},
   "source": [
    "#### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f21ee7",
   "metadata": {},
   "source": [
    "SVM tries to find a hyperplane defined by $w^T x + b = 0$ that maximizes the margin, which is inversely proportional to the norm of the weight vector (margain $ = \\frac{2}{||w||}$). It solves $\\min \\frac{1}{2}||w||^2$.\n",
    "If features are not scaled, the distance calculations are dominated by the feature with the larger range ($x_2$). To have a comparable effect on the decision boundary, the weight $w_2$ associated with the large feature would need to be very small, while $w_1$ would need to be large. Since the optimizer minimizes $||w||^2$, it favors solutions where the weights are small. So the optimization may focus almost only on $x_2$ to define the margin and ignore $x_1$, this might lead to inaccurate decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34afe7bb",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80d075f",
   "metadata": {},
   "source": [
    "Soft-margin SVM optimizes the function $$\\min \\frac{1}{2}||w||^2 + C \\sum_{i=1}^{N} \\xi_i.$$ In a highly imbalanced dataset, the majority of the contribution to this sum comes from the negative class. The model can minimize the objective function effectively by classifying all examples as negative (yielding $\\xi_i=0$ for 99% of the data). The penalty for misclassifying the 1% positive cases is smaller than the cost (reduction in margin or increase in negative misclassifications) required to correctly classify them. Thus, the model sacrifices the minority class to satisfy the majority."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbfddf5",
   "metadata": {},
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d681b7d5",
   "metadata": {},
   "source": [
    "To address the feature scaling issue, we could use min-max scaling as we did for question 1. Min-Max scaling rescales every feature to a fixed range, typically $[0, 1]$. For this question, we can map both $x_1$ and $x_2$ to the same $[0, 1]$ interval so that the size of the values doesn't throw off the optimization of the weight vector $w$. By scaling the data first, we ensure both features contribute equally to the calculation of the decision boundary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
